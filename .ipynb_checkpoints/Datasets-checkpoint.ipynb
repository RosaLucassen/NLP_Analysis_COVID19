{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis ADS: Rosa Lucassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re\n",
    "from collections import Counter\n",
    "import emoji\n",
    "import stop_words\n",
    "from scipy.stats import chi2_contingency\n",
    "from urllib.parse import urlparse\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "import pyLDAvis.gensim\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarity\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from numpy import array\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Phrases\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "d_test = pd.read_excel(\"/Users/rosalucassen/Desktop/Thesis/Code/Data/Constraint_English_Test.xlsx\")\n",
    "d_train = pd.read_excel(\"/Users/rosalucassen/Desktop/Thesis/Code/Data/Constraint_English_Train.xlsx\")\n",
    "d_val = pd.read_excel(\"/Users/rosalucassen/Desktop/Thesis/Code/Data/Constraint_English_Val.xlsx\")\n",
    "d_test_labeled = pd.read_excel(\"/Users/rosalucassen/Desktop/Thesis/Code/Data/english_test_with_labels.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_char(text):\n",
    "    single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "    return without_sc\n",
    "\n",
    "def remove_numbers(text):\n",
    "    number_pattern = r'\\b(?<![0-9-])(\\d+)(?![0-9-])'\n",
    "    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
    "    return without_number\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    without_urls = re.sub(pattern=url_pattern, repl=\" \", string=text)\n",
    "    return without_urls\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    without_emoji = emoji_pattern.sub(r'',text)\n",
    "    return without_emoji\n",
    "\n",
    "def remove_nonalpha(text):\n",
    "    nonalpha_pattern = \"[+$@#?~]\"\n",
    "    without_nonalpha = re.sub(pattern=nonalpha_pattern, repl=\" \", string=text)\n",
    "    return without_nonalpha\n",
    "    \n",
    "def remove_rt(text):\n",
    "    rt_pattern = 'rt @[\\w_]+: '\n",
    "    without_pattern = re.sub(pattern = rt_pattern, repl=\" \", string = text)\n",
    "    return without_pattern \n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    space_pattern = r'\\s+'\n",
    "    without_space = re.sub(pattern=space_pattern, repl=\" \", string=text)\n",
    "    return without_space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function BiGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "def make_n_grams(docs):\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "    bigram = Phrases(docs, min_count=20)\n",
    "    for idx in range(len(docs)):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Lemmatizing texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processer_real(corpus):\n",
    "    corpus = [text.lower() for text in corpus]\n",
    "    corpus = [remove_urls(text) for text in corpus]\n",
    "    corpus = [remove_single_char(text) for text in corpus]\n",
    "    corpus = [remove_numbers(text) for text in corpus]\n",
    "    corpus = [remove_rt(text) for text in corpus]\n",
    "    corpus = [remove_emojis(text) for text in corpus]\n",
    "    corpus = [remove_nonalpha(text) for text in corpus]\n",
    "    corpus = [text.encode(\"ascii\", \"ignore\") for text in corpus]\n",
    "    corpus = [text.decode() for text in corpus]\n",
    "    corpus = [remove_extra_spaces(text) for text in corpus]\n",
    "    #corpus = ' '.join(corpus).split()\n",
    "\n",
    "    processed_texts = [text for text in nlp.pipe(corpus, \n",
    "                                              disable=[\"ner\",\n",
    "                                                       \"parser\"])]\n",
    "    \n",
    "    \n",
    "    tokenized_texts = [[word.lemma_ for word in lemma_ if not word.is_punct if not word.is_stop] \n",
    "                            for lemma_ in processed_texts]\n",
    "    tokenized_texts = make_n_grams(tokenized_texts)\n",
    "    \n",
    "    \n",
    "    flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "    flat_real = flatten(tokenized_texts)\n",
    "    counts = Counter(flat_real)\n",
    "    total_words = sum(counts.values()) \n",
    "    for word in counts:\n",
    "        counts[word] = counts[word] * (10000) / total_words\n",
    "    return counts.most_common()[:20], flat_real, tokenized_texts\n",
    "\n",
    "def pre_processer_fake(corpus):\n",
    "    corpus = [text.lower() for text in corpus]\n",
    "    corpus = [remove_urls(text) for text in corpus]\n",
    "    corpus = [remove_single_char(text) for text in corpus]\n",
    "    corpus = [remove_numbers(text) for text in corpus]\n",
    "    corpus = [remove_rt(text) for text in corpus]\n",
    "    corpus = [remove_emojis(text) for text in corpus]\n",
    "    corpus = [remove_nonalpha(text) for text in corpus]\n",
    "    corpus = [text.encode(\"ascii\", \"ignore\") for text in corpus]\n",
    "    corpus = [text.decode() for text in corpus]\n",
    "    corpus = [remove_extra_spaces(text) for text in corpus]\n",
    "    #corpus = ' '.join(corpus).split()\n",
    "    \n",
    "    processed_texts = [text for text in nlp.pipe(corpus, \n",
    "                                              disable=[\"ner\",\n",
    "                                                       \"parser\"])]\n",
    "    tokenized_texts = [[word.lemma_ for word in lemma_ if not word.is_punct if not word.is_stop] \n",
    "                            for lemma_ in processed_texts]\n",
    "    tokenized_texts = make_n_grams(tokenized_texts)\n",
    "    \n",
    "    \n",
    "    flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "    flat_fake = flatten(tokenized_texts)\n",
    "    counts = Counter(flat_fake)\n",
    "    total_words = sum(counts.values()) \n",
    "    for word in counts:\n",
    "        counts[word] = counts[word] * (10000) / total_words\n",
    "    return counts.most_common()[:20], flat_fake, tokenized_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: distinctive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinctive_words(target_corpus, reference_corpus):\n",
    "    counts_c1 = Counter(target_corpus) # don't forget to flatten your texts!\n",
    "    counts_c2 = Counter(reference_corpus)\n",
    "    vocabulary = set(list(counts_c1.keys()) + list(counts_c2.keys()))\n",
    "    freq_c1_total = sum(counts_c1.values()) \n",
    "    freq_c2_total = sum(counts_c2.values()) \n",
    "    results = []\n",
    "    for word in vocabulary:\n",
    "        freq_c1 = counts_c1[word]\n",
    "        freq_c2 = counts_c2[word]\n",
    "        freq_c1_other = freq_c1_total - freq_c1\n",
    "        freq_c2_other = freq_c2_total - freq_c2\n",
    "        llr, p_value,_,_ = chi2_contingency([[freq_c1, freq_c2], \n",
    "                      [freq_c1_other, freq_c2_other]],\n",
    "                      lambda_='log-likelihood') \n",
    "        if freq_c2 / freq_c2_other > freq_c1 / freq_c1_other:\n",
    "            llr = -llr\n",
    "        result = {'word':word, \n",
    "                    'llr':llr,\n",
    "                    'p_value': p_value}\n",
    "        results.append(result)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Topic modelling LDA Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modelling(tokenized_text, n_topics, n_iterations):\n",
    "    results = []\n",
    "\n",
    "\n",
    "    dictionary = Dictionary(tokenized_text) # get the vocabulary\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "    PATH_TO_MALLET = '/Users/rosalucassen/Documents/UU/DataMining/mallet-2.0.8/bin/mallet'\n",
    "    N_TOPICS = n_topics\n",
    "    N_ITERATIONS = n_iterations\n",
    "\n",
    "    lda = LdaMallet(PATH_TO_MALLET,\n",
    "                    corpus=corpus,\n",
    "                    id2word=dictionary,\n",
    "                    num_topics=N_TOPICS,\n",
    "                    iterations=N_ITERATIONS)\n",
    "\n",
    "    for topic in range(N_TOPICS):\n",
    "        words = lda.show_topic(topic, 10)\n",
    "        topic_n_words = ' '.join([word[0] for word in words])\n",
    "        result = {'Topic':str(topic), \n",
    "                    'Words':topic_n_words}\n",
    "        results.append(result)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    return results_df, lda, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modelling_results(tokenized_text, n_topics, n_iterations):\n",
    "    results = []\n",
    "\n",
    "    dictionary = Dictionary(tokenized_text) # get the vocabulary\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "    PATH_TO_MALLET = '/Users/rosalucassen/Documents/UU/DataMining/mallet-2.0.8/bin/mallet'\n",
    "    N_TOPICS = n_topics\n",
    "    N_ITERATIONS = n_iterations\n",
    "\n",
    "    lda = LdaMallet(PATH_TO_MALLET,\n",
    "                    corpus=corpus,\n",
    "                    id2word=dictionary,\n",
    "                    num_topics=N_TOPICS,\n",
    "                    iterations=N_ITERATIONS)\n",
    "    \n",
    "    for topic in range(N_TOPICS):\n",
    "        words = lda.show_topic(topic)\n",
    "        topic_n_words = ' '.join([word[0] for word in words])\n",
    "        result = {'Topic':str(topic), \n",
    "                    'Words':topic_n_words}\n",
    "        results.append(result)\n",
    "    results_DF = pd.DataFrame(results)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    return results_DF, lda, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d_train[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_train = d_train.groupby('label').count() \n",
    "grouped_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_val[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_val = d_val.groupby('label').count() \n",
    "grouped_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test_labeled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test_labeled[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped_test = d_test_labeled.groupby('label').count() \n",
    "grouped_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
       "1   2  States reported 1121 deaths a small rise from ...  real\n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
       "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
       "4   5  Populous states can generate large case counts...  real"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.concat([d_train, d_val])\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8560 entries, 0 to 2139\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      8560 non-null   int64 \n",
      " 1   tweet   8560 non-null   object\n",
      " 2   label   8560 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 267.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>4080</td>\n",
       "      <td>4080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <td>4480</td>\n",
       "      <td>4480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  tweet\n",
       "label             \n",
       "fake   4080   4080\n",
       "real   4480   4480"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_train = data_train.groupby('label').count() \n",
    "grouped_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('coronavirus', 323.7707280518033), ('covid-19', 248.64054723978248), ('say', 86.46696001383471), ('claim', 80.51034721288165), (' ', 74.5537344119286), ('people', 73.59299041177488), ('virus', 65.71488961051438), ('trump', 59.75827680956132), ('vaccine', 59.18183040946909), ('new', 57.452491209192395), ('pandemic', 56.876044809100165), ('case', 55.91530080894645), ('covid19', 54.95455680879273), ('test', 53.80166400860826), ('video', 52.07232480833157), ('india', 51.688027208270086), ('hospital', 50.91943200814711), ('covid', 45.154968007224795), ('death', 44.19422400707108), ('show', 43.80992640700959)]\n",
      "[('case', 291.2221643679692), ('covid19', 214.08764515699357), ('test', 171.46386060085004), (' ', 156.81193465967573), ('report', 145.55053703546736), ('new', 140.7069251540874), ('state', 135.01568119346598), ('covid-19', 112.00852475691123), ('number', 105.2274681229793), ('death', 104.50092634077231), ('total', 87.18501386483901), ('confirm', 73.01744911180268), ('people', 72.53308792366468), ('amp', 66.11530218083625), ('day', 65.75203128973276), ('today', 62.119322378697795), ('testing', 57.63898138842134), ('update', 55.822626932903866), ('coronavirus', 55.33826574476587), ('indiafightscorona', 52.4320986159379)]\n"
     ]
    }
   ],
   "source": [
    "corpus_train_fake = data_train[data_train.label == \"fake\"][\"tweet\"]\n",
    "corpus_train_real = data_train[data_train.label == \"real\"][\"tweet\"]\n",
    "output_fake, flat_fake, tokenized_texts_fake = pre_processer_fake(corpus_train_fake)\n",
    "print(output_fake)\n",
    "output_real, flat_real, tokenized_texts_real = pre_processer_real(corpus_train_real)\n",
    "print(output_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Topic modelling LDA Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, lda_fake, dictionary = topic_modelling_results(tokenized_texts_fake, 8, 400)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score using c_v\n",
    "coherence_model_lda = CoherenceModel(model=lda_fake, texts=tokenized_texts_fake, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over documents to get topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_docs = lda_fake.load_document_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, document in enumerate(transformed_docs):\n",
    "    print('Topic distributions for document {}'.format(i))\n",
    "    for topic in document:\n",
    "        print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_docs = lda_fake.load_document_topics()\n",
    "topic_distributions = pd.DataFrame([[x[1] for x in doc] for doc in transformed_docs], \n",
    "             columns=['topic_{}'.format(i) for i in range(8)])\n",
    "topic_distributions.tail()\n",
    "topic_distributions.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_real, lda_real, dictionary = topic_modelling_results(tokenized_texts_real, 19, 1000)\n",
    "topics_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score using c_v\n",
    "coherence_model_lda = CoherenceModel(model=lda_real, texts=tokenized_texts_real, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_docs = lda_real.load_document_topics()\n",
    "for i, document in enumerate(transformed_docs):\n",
    "    print('Topic distributions for document {}'.format(i))\n",
    "    for topic in document:\n",
    "        print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_docs = lda_real.load_document_topics()\n",
    "topic_distributions = pd.DataFrame([[x[1] for x in doc] for doc in transformed_docs], \n",
    "             columns=['topic_{}'.format(i) for i in range(19)])\n",
    "topic_distributions.tail()\n",
    "#topic_distributions.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal number of topics method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(texts, limit, start=2, step=3):\n",
    "    dictionary = Dictionary(texts) # get the vocabulary\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    PATH_TO_MALLET = '/Users/rosalucassen/Documents/UU/DataMining/mallet-2.0.8/bin/mallet'\n",
    "    \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(PATH_TO_MALLET, corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the c_v measure\n",
    "model_list, coherence_values = compute_coherence_values(texts=tokenized_texts_fake, start=2, limit=40, step=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<gensim.models.wrappers.ldamallet.LdaMallet object at 0x7ff0ec679b20>, <gensim.models.wrappers.ldamallet.LdaMallet object at 0x7ff0c98c6e80>, <gensim.models.wrappers.ldamallet.LdaMallet object at 0x7ff0d9649850>, <gensim.models.wrappers.ldamallet.LdaMallet object at 0x7ff0c98c6b80>, <gensim.models.wrappers.ldamallet.LdaMallet object at 0x7ff0d8f78190>, <gensim.models.wrappers.ldamallet.LdaMallet object at 0x7ff0d96494f0>, <gensim.models.wrappers.ldamallet.LdaMallet object at 0x7ff0c98c6070>, <gensim.models.wrappers.ldamallet.LdaMallet object at 0x7ff0d8f78940>, <gensim.models.wrappers.ldamallet.LdaMallet object at 0x7ff0ec67f670>, <gensim.models.wrappers.ldamallet.LdaMallet object at 0x7ff0d8dd5fd0>] [0.4063896046189118, 0.40904552693503415, 0.3985240850218482, 0.39832554498207384, 0.37041717195598634, 0.3980514158610887, 0.4046139572542005, 0.3765709331774986, 0.3709879152410696, 0.40025054169255625]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Using the c_v measure\n",
    "#model_list, coherence_values = compute_coherence_values(texts=tokenized_texts_fake, start=2, limit=40, step=6)\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "limit=40; start=2; step=8;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score using UMass\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_texts_fake, dictionary=dictionary, coherence=\"u_mass\")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal number of topics method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This methods takes a very long time, but will also give alpha and beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tokenized_texts_fake, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[tokenized_texts_fake], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "PATH_TO_MALLET = '/Users/rosalucassen/Documents/UU/DataMining/mallet-2.0.8/bin/mallet'\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(tokenized_texts_fake)\n",
    "# Create Corpus\n",
    "texts = tokenized_texts_fake\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])\n",
    " \n",
    "# Build LDA model\n",
    "lda = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=7,\n",
    "                                       chunksize=500,\n",
    "                                       passes=20,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 10 topics\n",
    "#pprint(lda_real.print_topics())\n",
    "doc_lda = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=tokenized_texts_fake, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k,\n",
    "                                           chunksize=500,\n",
    "                                           passes=20,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "   \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_texts_fake, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 7\n",
    "max_topics = 10\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "#corpus = [dictionary.doc2bow(text) for text in tokenized_texts_real]\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=540)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results3.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Most distinctive words\n",
    "This gives an extreme output, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = distinctive_words(flat_real, flat_fake)\n",
    "results_df.sort_values('llr', ascending=False).head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
